todo list for JAX implementation of node pert:

- bring in CIFAR and SVHN datasets and make loaders for them
  - make these as similar as possible to the mnistloader case!
- try simple experiments with weight decay
- send message to JAX creators about how to work with convs in node perturbation
- make the node pert gradient function use the proper batched mse loss from losses
- refactor node perturbation nicely so that most of the gradient logic lives in fc.py rather than in nptest.py
- implement node pert for convolutions the simple way (i.e. without jac_rev)
  this may not be too difficult using einsum
- build functionality to monitor the variance in the gradients across np and sgd
- setup infrastructure for running experiments, tracking data, logging data, saving data
  - basic pickling should work fine. initial investigation says it looks good
  - need to write some functions for loading network parameters into jax variables
    because it looks like pickling makes everything into numpy variables
- make figure1.py code that loads experiment data and plots figure 1 for paper
- deprecate test.py and example.py which are both dead now
- make tool to build Noaki's figure
  loop over minibatch of 100
    linesearch for W + \alpha*dW
    for each point along the line search you compute the test loss difference
    


less critical infrastructure:
- fix the way that random numbers work everywhere


less critical experiments to run:

- build way to run node pert with a learned (rather than perfect) baseline
- try some experiments with learning rate schedule for node pert
- does adam help node pert?
- start from an sgd checkpoint and continue using node pert
  as a proxy for showing that starting from an unsupervised regime won't
  necessarily save node pert
- show that node pert is much more sensitive to the loss function and transfer functions


experimental notes:
- current code with 512-512-10 gets to 99.1% train accuracy and still climbing
  so the basic code is working well.
